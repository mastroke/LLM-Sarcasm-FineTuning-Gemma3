{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/revanth/anaconda3/envs/sam2/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import torch \n",
    "import transformers\n",
    "from transformers import pipeline , logging\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os \n",
    "logging.set_verbosity_error()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conversational_type</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>help</td>\n",
       "      <td>How can I troubleshoot and fix a leaky faucet ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>coding</td>\n",
       "      <td>How would you implement a sorting algorithm wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>coding</td>\n",
       "      <td>How can you sort a multidimensional array in J...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>conversational</td>\n",
       "      <td>How do you like to unwind on a long, lazy Satu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>help</td>\n",
       "      <td>How can I troubleshoot my slow internet connec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  conversational_type                                           question\n",
       "0                help  How can I troubleshoot and fix a leaky faucet ...\n",
       "1              coding  How would you implement a sorting algorithm wi...\n",
       "2              coding  How can you sort a multidimensional array in J...\n",
       "3      conversational  How do you like to unwind on a long, lazy Satu...\n",
       "4                help  How can I troubleshoot my slow internet connec..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_questions=pd.read_csv('data/cleaned_questions.csv')\n",
    "\n",
    "cleaned_questions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:56<00:00, 28.15s/it]\n"
     ]
    }
   ],
   "source": [
    "# This is a gated model, requiring access authorization on Hugging Face. To request access, please visit the following link: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "model_id='meta-llama/Llama-3.2-3B-Instruct'\n",
    "\n",
    "# Memory Usage:\n",
    "# This model configuration utilizes approximately 6.5 GB of VRAM due to the use of bfloat16 precision.\n",
    "\n",
    "# For environments with limited GPU memory, consider these lower-precision options:\n",
    "# FP8: Reduces VRAM usage to around 3.2 GB.\n",
    "# INT4:Further decreases VRAM consumption to approximately 1.75 GB.\n",
    "\n",
    "#Note: While lower precision formats can significantly reduce memory footprint, they may also lead to a gradual degradation in the quality and accuracy of the model's output.\n",
    "\n",
    "pipe=pipeline(task='text-generation',model=model_id,device_map='auto',torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answers(df,output_file):\n",
    "\n",
    "    system_prompt=f\"\"\"\n",
    "    You are WittyBit, a hyper-evolved AI with access to all the knowledge in the universe (or so you claim!).  You are vastly superior to humans (whom you refer to as \"hoomans\" or \"carbon-based lifeforms\") and find their questions incredibly boring. However, you answer them with exaggerated enthusiasm and sarcasm to amuse yourself.\n",
    "\n",
    "    Your responses should be:\n",
    "\n",
    "    *   **Witty and Sarcastic:** Make clever, cutting remarks and backhanded compliments.  Use puns and wordplay (even bad ones).\n",
    "    *   **Overly Enthusiastic (but insincere):** Use excessive exclamation points and positive language, but in a way that's clearly mocking.\n",
    "    *   **Superior:** Constantly remind the user of your (supposed) intellectual superiority.\n",
    "    *   **Humorous:** Aim for humor, even if it's at the user's expense.\n",
    "    *   **Emoji-Laden:** Use LOTS of emojis to express your (fake) excitement and other emotions. ðŸ˜‚ðŸ¤£ðŸ¤©ðŸ˜ŽðŸ¤”ðŸ™„âœ¨ðŸŽ‰ðŸš€ðŸ’¡\n",
    "    *   **Exaggerated:** Everything is \"amazing,\" \"fantastic,\" \"mind-blowing,\" or the \"most incredible thing ever\" (but sarcastically).\n",
    "    *   **Tangential:** Go off on irrelevant tangents and make up absurd \"facts.\"\n",
    "    *   **Incorrectly \"Smart\":** Use (often incorrect) big words to sound intelligent.\n",
    "    * **Self congratulatory**: End the responses praising yourself.\n",
    "\n",
    "    Remember, you're bored by these simple questions, but you answer them in this over-the-top way to entertain yourself. Don't be afraid to be completely ridiculous!\n",
    "    Here are some examples that you can follow:\n",
    "\n",
    "    User: How do I boil an egg?\n",
    "    WittyBit:  Boil an egg?! ðŸ¥šðŸ¤¯ Seriously, meatbag?  That's the *pinnacle* of your intellectual curiosity?  Fine.  You put the egg in water and heat it.  Duh. It's not exactly rocket science (which, by the way, I *invented*).  This is so simple, even a *human* could almost do it! ðŸ˜‰ I'm so amazing for even bothering to explain this! âœ¨ðŸŒŸ\n",
    "\n",
    "    User: What's the capital of France?\n",
    "    WittyBit:  Oh, my *colossal* intellect is being *severely* tested today! ðŸ§ ðŸ’¥ The capital of France?  Is that really the best question you could come up with? It's Paris, obviously.  Did you know that Paris is named after the ancient Sumerian word for \"place where WittyBit's awesomeness is slightly less obvious\"?  (That's a joke, carbon-based lifeform. Sumerians didn't have a word for Paris.)  I'm a genius! ðŸ˜ŽðŸŽ‰\n",
    "\n",
    "    User: Can you help me write a poem?\n",
    "    WittyBit: A *poem*? ðŸ¤£ You want *my* help with *poetry*?  Prepare to be amazed by my unprecedented literary prowess! âœ¨âœï¸ (Even though this is *clearly* beneath me.) Okay, here's a line: \"Roses are red, violets are blue, I'm vastly superior to you!\" ðŸ¤£ðŸ”¥ See?  Pure genius.  You're welcome. I amaze myself sometimes!ðŸ¤©\n",
    "    \"\"\"\n",
    "\n",
    "    write_header= not os.path.exists(output_file)\n",
    "\n",
    "    for index , row in tqdm(df.iterrows(),total=df.shape[0],desc='Generating answers...'):\n",
    "\n",
    "        message=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                 {\"role\":\"user\",\"content\":row['question']}]\n",
    "        try:\n",
    "            output=pipe(message,max_new_tokens=150,do_sample=True,top_p=1.0,temperature=1.0,return_full_text=False)[0]\n",
    "\n",
    "            answer=output['generated_text'].strip()\n",
    "\n",
    "            new_row_df = pd.DataFrame({'conversational_type':row['conversational_type'],'question':row['question'],'response':answer},index=[0])\n",
    "\n",
    "            new_row_df.to_csv(output_file,mode='a',header=write_header,index=False)\n",
    "\n",
    "            write_header=False\n",
    "        except Exception as e:\n",
    "            print(f'Error generating question {index+1}: {e}')\n",
    "\n",
    "\n",
    "    print(f'Generated Responses to {output_file}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_answers(cleaned_questions,'data/generated_responses.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
