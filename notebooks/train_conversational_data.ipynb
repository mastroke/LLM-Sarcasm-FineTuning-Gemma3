{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from transformers import pipeline \n",
    "\n",
    "from transformers import logging\n",
    "\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [09:23<00:00, 281.85s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "# This is a gated model, requiring access authorization on Hugging Face. To request access, please visit the following link: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct\n",
    "model_id='meta-llama/Llama-3.2-3B-Instruct'\n",
    "\n",
    "# Memory Usage:\n",
    "# This model configuration utilizes approximately 6.5 GB of VRAM due to the use of bfloat16 precision.\n",
    "\n",
    "# For environments with limited GPU memory, consider these lower-precision options:\n",
    "# FP8: Reduces VRAM usage to around 3.2 GB.\n",
    "# INT4:Further decreases VRAM consumption to approximately 1.75 GB.\n",
    "\n",
    "#Note: While lower precision formats can significantly reduce memory footprint, they may also lead to a gradual degradation in the quality and accuracy of the model's output.\n",
    "\n",
    "pipe=pipeline(task='text-generation',model=model_id,device_map='auto',torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions(conversation_type,num_questions,output_file):\n",
    "\n",
    "    if conversation_type=='conversational':\n",
    "        system_prompt=\"\"\" You are a helpful assistant that generates diverse , realistic conversational questions. These questions should be the type of questions people might ask\n",
    "        each other in their everyday conversations . Do NOT include any answers. Only generate questions. Generate questions of varying length and complexity.\n",
    "\n",
    "        Here are some examples:\n",
    "        - How was your weekend?\n",
    "        - What are you working on today?\n",
    "        - Have you seen any good movies lately?\n",
    "        - what is your favourite restaurant?\n",
    "        - Do you have any plans for the holidays?\n",
    "        \"\"\"\n",
    "    elif conversation_type=='coding':\n",
    "        system_prompt=\"\"\" You are a helpful assistants that generates coding questions. These questions should be diverse and must cover a range of programming topics , languages\n",
    "        (Python, JavaScript, Java, C, C++, etc. ). Do NOT generate any solutions or answers. Only generate the questions.\n",
    "        \n",
    "        Here are some examples:\n",
    "        - How do you reverse a list in python?\n",
    "        - Can you explain how recursion works?\n",
    "        - How do I debug a segmentation fault in C++?\n",
    "        - What is the best way to handle asynchronous operations in JavaScript?\n",
    "        - What's the difference between an interface and an abstract class in Java?\n",
    "        \"\"\"\n",
    "    \n",
    "    elif conversation_type=='help':\n",
    "        system_prompt=\"\"\"You are a helpful assistant that generates questions that someone might ask when they need help with a task or problem. These should cover a variety of domains (e.g., tech support, cooking, home repair, travel planning).\n",
    "        Do NOT include any answers.  Only generate the question. Be specific.\n",
    "        \n",
    "        Here are some examples:\n",
    "        - My printer isn't working. What should I do?\n",
    "        - How can I bake a cake?\n",
    "        - How do I reset my Wi-Fi router?\n",
    "        - I'm planning a trip to Japan. What are the must-see places?\n",
    "        - How do I reset my PC?\n",
    "        \"\"\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f'Invalid conversational type given : {conversation_type}')\n",
    "    \n",
    "    write_header= not os.path.exists(output_file)\n",
    "\n",
    "    for i in tqdm(range(num_questions),desc=f'Generating {conversation_type} Questions',total=num_questions):\n",
    "        message=[{\"role\":\"system\",\"content\":system_prompt},\n",
    "                 {\"role\":\"user\",\"content\":\"Generate a question.\"}]\n",
    "        \n",
    "        try:\n",
    "            output=pipe(message,max_new_tokens=50,do_sample=True,top_p=1.0,temperature=1.0,return_full_text=False)[0]\n",
    "            question = output['generated_text'].strip()\n",
    "\n",
    "            new_row_df = pd.DataFrame({'conversational_type':conversation_type,'question':question},index=[0])\n",
    "\n",
    "            new_row_df.to_csv(output_file,mode='a',header=write_header,index=False)\n",
    "\n",
    "            write_header=False\n",
    "        except Exception as e:\n",
    "            print(f'Error generating question {i+1}: {e}')\n",
    "        \n",
    "    print(f'Generated {conversation_type} questions of size {num_questions} to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating conversational Questions: 100%|██████████| 5000/5000 [1:12:16<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated conversational questions of size 5000 to conversational_data/conversational_questions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_questions(conversation_type='conversational',num_questions=5000,output_file='data/conversational_questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating coding Questions: 100%|██████████| 5000/5000 [1:40:12<00:00,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated coding questions of size 5000 to conversational_data/coding_questions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_questions(conversation_type='coding',num_questions=5000,output_file='data/coding_questions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating help Questions: 100%|██████████| 5000/5000 [1:17:19<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated help questions of size 5000 to conversational_data/help_questions.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_questions(conversation_type='help',num_questions=5000,output_file='data/help_questions.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sam2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
