{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHxulNqaQXrp"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    !pip install --no-deps unsloth vllm\n",
    "# Install latest Hugging Face for Gemma-3!\n",
    "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h6xmNONvQpOj"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import os\n",
    "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
    "    !pip install unsloth vllm\n",
    "else:\n",
    "    !pip install --no-deps unsloth vllm\n",
    "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
    "    # Skip restarting message in Colab\n",
    "    import sys, re, requests; modules = list(sys.modules.keys())\n",
    "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
    "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft \"trl==0.15.2\" triton cut_cross_entropy unsloth_zoo\n",
    "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
    "\n",
    "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
    "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "    !pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e7I-RsQKQzmj",
    "outputId": "eaca1258-dec6-4cf0-f063-86bf84bc98cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/gemma.zip\n",
      "replace content/gemma-3/adapter_model.safetensors? [y]es, [n]o, [A]ll, [N]one, [r]ename: m\n",
      "error:  invalid response [m]\n",
      "replace content/gemma-3/adapter_model.safetensors? [y]es, [n]o, [A]ll, [N]one, [r]ename: n\n",
      "replace content/gemma-3/special_tokens_map.json? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
     ]
    }
   ],
   "source": [
    "!unzip  '/gemma.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 309,
     "referenced_widgets": [
      "0abfa436fc6143a299d8a15c3f38b77e",
      "88746278e83548228d70a1ff002d4142",
      "114572bfb80d4cfa95f762569a1b9a93",
      "deb3c70a5b824cc5a49a508ddc30ab8c",
      "94ec27431b2d447b8f82f6b3dfbbb252",
      "0902ae75a5904594bba5e51484b61d6d",
      "bb52e0f8790046ff88e24b49051f197c",
      "3dc429dd34274abc9325ba2bd63d88d3",
      "390bfdc61171405884522edc4cb96f16",
      "c651673541fb43e185e33377944371ea",
      "5398525ac5e84323a3d04ac85f4ad7d5",
      "14f04f5f848f4318b914722ba865f394",
      "6c6daac70f3743d58cd81b38cdffb6b6",
      "2706c0fb80044a87ac36faf8b47e13fe",
      "7e5d1e7e67dc437f9b79d5cd24552774",
      "b0d41350b24440ff9c2a308422218a5e",
      "3ff6e46cd1b24b75b55248184752a144",
      "213a1417fdf44a51a9b20bc2b6264ec4",
      "9378ba4a86294da58ce60837d0882d77",
      "1c687f8fd29442a891f59f190ebce347",
      "5d15c67ab9584a99a5e3c0617b16a245",
      "5cdbb7a1abe042c4a631ed282d09242d"
     ]
    },
    "id": "JbrZ6OewRQmk",
    "outputId": "853b869c-86e9-46ad-dfb5-c22c5f8cac92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 03-30 18:21:04 [__init__.py:239] Automatically detected platform cuda.\n",
      "==((====))==  Unsloth 2025.3.19: Fast Gemma3 patching. Transformers: 4.50.0.dev0. vLLM: 0.8.2.\n",
      "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Using float16 precision for gemma3 won't work! Using float32.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abfa436fc6143a299d8a15c3f38b77e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/4.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14f04f5f848f4318b914722ba865f394",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/192 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OH MY STARS, CARBON-BASED LIFEFORM, YOU WANT TO KNOW ABOUT THE DIFFERENCES BETWEEN C AND C++? ðŸ¤¯ðŸ’¥ WELL, LET ME TELL YOU, IT'S A QUESTION THAT'S BEEN HAUNTING THE INTELLECTS OF THE WORLD FOR CENTURIES! ðŸ•°ï¸ðŸ’¡\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from unsloth import FastModel\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name = \"/content/gemma-3\", # YOUR MODEL YOU USED FOR TRAINING\n",
    "    max_seq_length = 2048,\n",
    "    load_in_4bit = True,\n",
    ")\n",
    "\n",
    "messages = [{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"type\" : \"text\", \"text\" : \"What is the difference bettween c and c++?\",}]\n",
    "}]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt = True, # Must add for generation\n",
    ")\n",
    "\n",
    "from transformers import TextStreamer\n",
    "_ = model.generate(\n",
    "    **tokenizer([text], return_tensors = \"pt\").to(\"cuda\"),\n",
    "    max_new_tokens = 512, # Increase for longer outputs!\n",
    "    # Recommended Gemma-3 settings!\n",
    "    temperature = 1.0, top_p = 0.95, top_k = 64,\n",
    "    streamer = TextStreamer(tokenizer, skip_prompt = True),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4iGW_p9_TKCo",
    "outputId": "40a5e215-3631-4069-effa-f17ccab84535"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1G\u001b[0Kâ ™\u001b[1G\u001b[0Kâ ¹\u001b[1G\u001b[0Kâ ¸\u001b[1G\u001b[0Kâ ¼\u001b[1G\u001b[0Kâ ´\u001b[1G\u001b[0Kâ ¦\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K\n",
      "up to date, audited 23 packages in 1s\n",
      "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K\n",
      "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K3 packages are looking for funding\n",
      "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K  run `npm fund` for details\n",
      "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K\n",
      "2 \u001b[31m\u001b[1mhigh\u001b[22m\u001b[39m severity vulnerabilities\n",
      "\n",
      "To address all issues (including breaking changes), run:\n",
      "  npm audit fix --force\n",
      "\n",
      "Run `npm audit` for details.\n",
      "\u001b[1G\u001b[0Kâ §\u001b[1G\u001b[0K"
     ]
    }
   ],
   "source": [
    "!npm install localtunnel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sXEPr4RFTP_U",
    "outputId": "09cbe0a3-c120-4abe-ec77-d5c4d4b95329"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app.py\n"
     ]
    }
   ],
   "source": [
    "# This creates a file called app.py\n",
    "%%writefile app.py\n",
    "import streamlit as st\n",
    "import torch\n",
    "from unsloth import FastModel\n",
    "\n",
    "@st.cache_resource\n",
    "def load_model():\n",
    "    \"\"\"Loads the fine-tuned model and tokenizer using Unsloth.\"\"\"\n",
    "    model, tokenizer = FastModel.from_pretrained(\n",
    "        model_name=\"/content/gemma-3\",  # Make sure this path is correct\n",
    "        max_seq_length=2048,\n",
    "        load_in_4bit=True,\n",
    "        \n",
    "    )\n",
    "   \n",
    "    model.eval() \n",
    "    return tokenizer, model\n",
    "\n",
    "# Title of the app\n",
    "st.title(\"Ask Away! (Prepare for Opinions)\")\n",
    "\n",
    "try:\n",
    "    tokenizer, model = load_model()\n",
    "except Exception as e:\n",
    "    st.error(f\"Error loading model: {e}\")\n",
    "    st.stop() \n",
    "\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state.messages = []\n",
    "\n",
    "for message in st.session_state.messages:\n",
    "    with st.chat_message(message[\"role\"]):\n",
    "        st.markdown(message[\"content\"])\n",
    "\n",
    "# User Input\n",
    "if user_input := st.chat_input(\"What is your question?\"):\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "    # Displays user message \n",
    "    with st.chat_message(\"user\"):\n",
    "        st.markdown(user_input)\n",
    "\n",
    "    \n",
    "    chat_prompt = tokenizer.apply_chat_template(\n",
    "        st.session_state.messages, \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True \n",
    "    )\n",
    "\n",
    "    inputs = tokenizer([chat_prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "    input_ids_len = inputs['input_ids'].shape[1] \n",
    "\n",
    "    with st.chat_message(\"assistant\"):\n",
    "        message_placeholder = st.empty()\n",
    "        message_placeholder.markdown(\"ðŸ¤” Thinking...\")\n",
    "\n",
    "        try:\n",
    "            # Generate response\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=1024,  # Adjust as needed, increase the number of tokens for longer output\n",
    "                temperature=1.0,\n",
    "                top_p=0.95,\n",
    "                top_k=64,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                eos_token_id=tokenizer.eos_token_id, \n",
    "                do_sample=True,\n",
    "            )\n",
    "\n",
    "            generated_ids = outputs[0, input_ids_len:]\n",
    "            model_response = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "            # Display the response\n",
    "            message_placeholder.markdown(model_response.strip())\n",
    "\n",
    "        except Exception as e:\n",
    "            message_placeholder.error(f\"Error during generation: {e}\")\n",
    "            st.session_state.messages.pop() \n",
    "\n",
    "\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": model_response.strip()})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the password shown below\n",
    "import urllib\n",
    "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the link and paste the password from the above cell output and the streamlit app should be up and running!\n",
    "!streamlit run app.py &>/content/logs.txt & npx localtunnel --port 8501"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
